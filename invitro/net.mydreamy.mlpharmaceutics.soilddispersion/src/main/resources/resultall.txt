       MultiLayerNetwork net = new MultiLayerNetwork(new NeuralNetConfiguration.Builder()
                .seed(seed)
                .iterations(iterations)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .learningRate(learningRate)
                .weightInit(WeightInit.RELU)
                .regularization(true)
                .l2(1e-3)
                .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer)
              //  .dropOut(0.5)
                .updater(Updater.NESTEROVS).momentum(0.8)
              //  .updater(Updater.ADAM)
                .list()
                .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(1, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(2, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(3, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(4, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(5, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(6, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                        .activation("softmax")
                        .nIn(numHiddenNodes).nOut(numOutputs).build())
                .pretrain(false).backprop(true).build()
                
                  public static final double learningRate = 0.01;
    
    //with api properties
    public static final int numInputs = 15;
    //
    //public static final int numInputs = 18;
    public static final int numOutputs = 2;
    public static final int numHiddenNodes = 250;
    
    [main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - New best model: score = 0.6019868678303287, epoch = 4000 (previous: score = 0.6024111711642296, epoch = 3900)
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 4000. Details: MaxEpochsTerminationCondition(4000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(4000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 4001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 4000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.6019868678303287
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ===========INPUT===================
[[0.27, 0.41, 0.46, 0.25, 0.27, 0.33, 0.47, 0.25, 0.27, 0.18, 0.64, 0.09, 0.00, 0.18, 0.50],
 [0.00, 1.00, 0.24, 0.00, 0.18, 0.67, 0.18, 0.38, 0.26, 0.28, 0.07, 0.07, 1.00, 0.18, 0.00],
 [0.74, 0.92, 0.56, 0.50, 0.73, 0.92, 0.79, 0.72, 0.74, 0.22, 0.00, 0.07, 1.00, 0.31, 0.75],
 [0.23, 0.44, 0.53, 0.25, 0.18, 0.42, 0.45, 0.16, 0.16, 0.22, 0.79, 0.01, 1.00, 0.02, 0.00],
 [0.25, 0.53, 0.25, 0.25, 0.55, 0.42, 0.76, 0.25, 0.39, 0.23, 0.57, 0.01, 0.00, 0.31, 0.33],
 [0.27, 0.41, 0.46, 0.25, 0.27, 0.33, 0.47, 0.25, 0.27, 0.18, 0.86, 0.05, 0.00, 0.31, 0.75],
 [0.00, 1.00, 0.24, 0.00, 0.18, 0.67, 0.18, 0.38, 0.26, 0.28, 0.14, 0.05, 1.00, 0.40, 0.00],
 [1.00, 0.49, 0.63, 0.00, 0.73, 0.92, 0.00, 1.00, 1.00, 0.19, 0.07, 0.03, 1.00, 0.18, 0.60],
 [0.00, 1.00, 0.24, 0.00, 0.18, 0.67, 0.18, 0.38, 0.26, 0.28, 0.07, 0.07, 1.00, 0.74, 0.00],
 [0.01, 0.54, 0.10, 0.50, 0.18, 0.17, 0.52, 0.00, 0.07, 0.32, 0.86, 0.18, 1.00, 0.27, 0.00],
 [0.74, 0.92, 0.56, 0.50, 0.73, 0.92, 0.79, 0.72, 0.74, 0.22, 1.00, 0.07, 1.00, 0.31, 0.75],
 [0.02, 0.72, 0.23, 0.25, 0.00, 0.00, 0.31, 0.03, 0.06, 0.30, 0.29, 0.13, 0.00, 0.31, 0.00],
 [0.00, 0.42, 0.33, 0.25, 0.18, 0.25, 0.32, 0.00, 0.00, 0.24, 0.86, 0.09, 1.00, 0.18, 0.75],
 [0.27, 0.41, 0.46, 0.25, 0.27, 0.33, 0.47, 0.25, 0.27, 0.18, 0.00, 0.15, 1.00, 0.18, 0.75],
 [0.25, 0.53, 0.25, 0.25, 0.55, 0.42, 0.76, 0.25, 0.39, 0.23, 0.57, 0.01, 0.00, 0.31, 0.11]]
=================OUTPUT==================
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.49, 0.51],
 [0.54, 0.46],
 [0.57, 0.43],
 [0.55, 0.45],
 [0.50, 0.50],
 [0.48, 0.52],
 [0.52, 0.48],
 [0.54, 0.46],
 [0.55, 0.45],
 [0.48, 0.52],
 [0.54, 0.46],
 [0.49, 0.51],
 [0.48, 0.52],
 [0.53, 0.47],
 [0.51, 0.49]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.07971014492753623
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.06666666666666667





                .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(1, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(2, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(3, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(4, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(5, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(6, new OutputLayer.Builder(LossFunctions.LossFunction.L2)
                        .activation("identity")
                        .nIn(numHiddenNodes).nOut(numOutputs).build())
                .pretrain(false).backprop(true).build()


[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(10000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 10001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1200
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.5146534071704523
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.18, 0.68],
 [0.56, -0.18],
 [0.98, 0.94],
 [1.50, 1.15],
 [-0.04, 0.05],
 [-0.03, -0.02],
 [0.34, 0.10],
 [1.16, 0.98],
 [-0.09, -0.01],
 [0.79, 0.79],
 [1.13, 1.31],
 [0.07, 0.12],
 [-0.05, -0.14],
 [-0.20, -0.10],
 [-0.03, 0.05]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9927536231884058
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6666666666666666
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.22, 0.46],
 [0.61, 0.24],
 [1.09, 0.91],
 [1.51, 1.42],
 [-0.02, 0.19],
 [0.02, 0.03],
 [0.48, 0.30],
 [1.06, 1.10],
 [-0.02, -0.08],
 [0.77, 0.77],
 [0.71, 0.94],
 [-0.17, -0.12],
 [0.07, 0.02],
 [0.42, 0.18],
 [-0.05, 0.09]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9420289855072463
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6



        MultiLayerNetwork net = new MultiLayerNetwork(new NeuralNetConfiguration.Builder()
                .seed(seed)
                .iterations(iterations)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .learningRate(learningRate)
                .weightInit(WeightInit.RELU)
                .regularization(true)
                .l2(1e-3)
                .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer)
              //  .dropOut(0.5)
                .updater(Updater.NESTEROVS).momentum(0.8)
              //  .updater(Updater.ADAM)
                .list()
                .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(1, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(2, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(3, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(4, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(5, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(6, new OutputLayer.Builder(LossFunctions.LossFunction.L2)
                        .activation("sigmoid")
                        .nIn(numHiddenNodes).nOut(numOutputs).build())
                .pretrain(false).backprop(true).build()

[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 2001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 400
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.4884929520190842
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.01, 0.04],
 [0.68, 0.45],
 [1.00, 0.99],
 [1.00, 0.99],
 [0.01, 0.04],
 [0.00, 0.01],
 [0.45, 0.31],
 [1.00, 0.93],
 [0.02, 0.04],
 [0.74, 0.68],
 [0.99, 0.98],
 [0.01, 0.01],
 [0.04, 0.04],
 [0.72, 0.47],
 [0.01, 0.04]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8986
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.20, 0.23],
 [0.66, 0.51],
 [0.69, 0.66],
 [0.84, 0.70],
 [0.27, 0.30],
 [0.15, 0.16],
 [0.63, 0.44],
 [0.78, 0.68],
 [0.53, 0.32],
 [0.57, 0.52],
 [0.63, 0.64],
 [0.17, 0.17],
 [0.29, 0.33],
 [0.49, 0.40],
 [0.34, 0.38]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7101
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333

        MultiLayerNetwork net = new MultiLayerNetwork(new NeuralNetConfiguration.Builder()
                .seed(seed)
                .iterations(iterations)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .learningRate(learningRate)
                .weightInit(WeightInit.RELU)
                .regularization(true)
                .l2(1e-3)
                .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer)
              //  .dropOut(0.5)
                .updater(Updater.NESTEROVS).momentum(0.8)
              //  .updater(Updater.ADAM)
                .list()
                .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(1, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(2, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(3, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(4, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(5, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(6, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(7, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(8, new DenseLayer.Builder().nIn(numHiddenNodes).nOut(numHiddenNodes)
                        .activation("tanh")
                        .build())
                .layer(9, new OutputLayer.Builder(LossFunctions.LossFunction.L2)
                        .activation("sigmoid")
                        .nIn(numHiddenNodes).nOut(numOutputs).build())
                .pretrain(false).backprop(true).build()

[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.5489672051843366
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.97, 0.66],
 [0.90, 0.81],
 [0.41, 0.24],
 [0.99, 0.99],
 [0.01, 0.04],
 [0.09, 0.10],
 [0.03, 0.02],
 [0.18, 0.12],
 [0.01, 0.01],
 [0.02, 0.01],
 [0.94, 0.92],
 [0.03, 0.04],
 [0.98, 0.96],
 [0.72, 0.54],
 [0.07, 0.05]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9203
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.97, 0.66],
 [0.90, 0.81],
 [0.41, 0.24],
 [0.99, 0.99],
 [0.01, 0.04],
 [0.09, 0.10],
 [0.03, 0.02],
 [0.18, 0.12],
 [0.01, 0.01],
 [0.02, 0.01],
 [0.94, 0.92],
 [0.03, 0.04],
 [0.98, 0.96],
 [0.72, 0.54],
 [0.07, 0.05]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9203
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333

[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - New best model: score = 0.4580779628578649, epoch = 2000 (previous: score = 0.4618850516444589, epoch = 1900)
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 2000. Details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 2001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 2000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.4580779628578649
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.99, 0.79],
 [0.96, 0.90],
 [0.03, 0.00],
 [1.00, 1.00],
 [0.00, 0.04],
 [0.01, 0.02],
 [0.00, 0.00],
 [0.01, 0.01],
 [0.03, 0.07],
 [0.00, 0.00],
 [1.00, 0.99],
 [0.31, 0.66],
 [0.99, 0.98],
 [0.56, 0.40],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9783
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.99, 0.79],
 [0.96, 0.90],
 [0.03, 0.00],
 [1.00, 1.00],
 [0.00, 0.04],
 [0.01, 0.02],
 [0.00, 0.00],
 [0.01, 0.01],
 [0.03, 0.07],
 [0.00, 0.00],
 [1.00, 0.99],
 [0.31, 0.66],
 [0.99, 0.98],
 [0.56, 0.40],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9783
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000

[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(20000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 20001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 2800
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.44134157881071395
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.88, 1.00],
 [1.00, 1.00],
 [1.00, 0.97],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 1.0000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[1.00, 0.92],
 [0.99, 0.95],
 [0.01, 0.00],
 [1.00, 1.00],
 [0.00, 0.01],
 [0.00, 0.01],
 [0.00, 0.00],
 [0.00, 0.01],
 [0.01, 0.05],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.55, 0.90],
 [1.00, 0.99],
 [0.68, 0.50],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9855
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000

 public static final int numHiddenNodes = 200;
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(3000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 3001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1500
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.5411623001090622
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[1.00, 0.73],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.98],
 [0.71, 0.57],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9855
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.99, 0.70],
 [0.99, 0.99],
 [0.20, 0.04],
 [1.00, 0.99],
 [0.00, 0.01],
 [0.06, 0.04],
 [0.01, 0.01],
 [0.04, 0.04],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.97, 0.93],
 [0.00, 0.01],
 [1.00, 0.99],
 [0.70, 0.45],
 [0.01, 0.02]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9275
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000

--------------- random -------------

main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 3000 is 0.02565538878604017
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 3000
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 3000. Details: MaxEpochsTerminationCondition(3000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(3000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 3001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 200
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.5795995338729688
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.16, 0.62],
 [0.43, 0.01],
 [1.00, 0.99],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.01, 0.01],
 [0.08, 0.01],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.01, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9928
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6667
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.28, 0.22],
 [0.62, 0.51],
 [0.69, 0.62],
 [0.81, 0.65],
 [0.23, 0.26],
 [0.25, 0.18],
 [0.60, 0.46],
 [0.79, 0.65],
 [0.54, 0.38],
 [0.60, 0.53],
 [0.57, 0.66],
 [0.40, 0.20],
 [0.50, 0.39],
 [0.51, 0.35],
 [0.26, 0.32]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.6812
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6000

.weightInit(WeightInit.XAVIER)
[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 1500 is 0.3423270772958611
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 1500
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 1500. Details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 1501
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1300
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.4296087624919998
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.17, 0.13],
 [0.73, 0.62],
 [0.82, 0.73],
 [0.84, 0.76],
 [0.24, 0.19],
 [0.10, 0.08],
 [0.66, 0.54],
 [0.95, 0.90],
 [0.49, 0.38],
 [0.64, 0.54],
 [0.89, 0.81],
 [0.11, 0.09],
 [0.24, 0.19],
 [0.35, 0.28],
 [0.34, 0.27]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7101
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.22, 0.18],
 [0.70, 0.59],
 [0.69, 0.58],
 [0.78, 0.68],
 [0.26, 0.21],
 [0.16, 0.13],
 [0.66, 0.55],
 [0.85, 0.77],
 [0.58, 0.47],
 [0.67, 0.57],
 [0.78, 0.67],
 [0.20, 0.16],
 [0.38, 0.31],
 [0.42, 0.34],
 [0.33, 0.27]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.6522
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6667

public static final int numHiddenNodes = 500;
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 1500. Details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 1501
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1200
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.47283319703479076
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.17, 0.12],
 [0.75, 0.65],
 [0.87, 0.80],
 [0.85, 0.78],
 [0.25, 0.18],
 [0.09, 0.07],
 [0.67, 0.56],
 [0.97, 0.95],
 [0.48, 0.37],
 [0.60, 0.50],
 [0.91, 0.86],
 [0.10, 0.08],
 [0.23, 0.17],
 [0.37, 0.29],
 [0.36, 0.27]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7174
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6667
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.25, 0.19],
 [0.68, 0.60],
 [0.68, 0.60],
 [0.74, 0.67],
 [0.28, 0.22],
 [0.20, 0.15],
 [0.65, 0.57],
 [0.83, 0.78],
 [0.58, 0.49],
 [0.63, 0.54],
 [0.75, 0.67],
 [0.22, 0.17],
 [0.44, 0.35],
 [0.47, 0.39],
 [0.34, 0.26]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.6449
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6667

numHiddenNodes = 300;
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 1500. Details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 1501
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1300
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.4296087624919998
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.17, 0.13],
 [0.73, 0.62],
 [0.82, 0.73],
 [0.84, 0.76],
 [0.24, 0.19],
 [0.10, 0.08],
 [0.66, 0.54],
 [0.95, 0.90],
 [0.49, 0.38],
 [0.64, 0.54],
 [0.89, 0.81],
 [0.11, 0.09],
 [0.24, 0.19],
 [0.35, 0.28],
 [0.34, 0.27]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7101
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.22, 0.18],
 [0.70, 0.59],
 [0.69, 0.58],
 [0.78, 0.68],
 [0.26, 0.21],
 [0.16, 0.13],
 [0.66, 0.55],
 [0.85, 0.77],
 [0.58, 0.47],
 [0.67, 0.57],
 [0.78, 0.67],
 [0.20, 0.16],
 [0.38, 0.31],
 [0.42, 0.34],
 [0.33, 0.27]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.6522
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6667

300 nodes, random, XIVA
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(5000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 5001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 5000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.41514721553770756
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.17],
 [1.00, 0.80],
 [1.00, 0.78],
 [1.00, 0.83],
 [0.00, 0.21],
 [0.00, 0.12],
 [1.00, 0.68],
 [1.00, 0.90],
 [0.00, 0.03],
 [1.00, 0.86],
 [1.00, 0.79],
 [0.00, 0.19],
 [0.00, 0.19],
 [0.00, 0.18],
 [0.00, 0.08]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8261
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.17],
 [1.00, 0.80],
 [1.00, 0.78],
 [1.00, 0.83],
 [0.00, 0.21],
 [0.00, 0.12],
 [1.00, 0.68],
 [1.00, 0.90],
 [0.00, 0.03],
 [1.00, 0.86],
 [1.00, 0.79],
 [0.00, 0.19],
 [0.00, 0.19],
 [0.00, 0.18],
 [0.00, 0.08]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8261
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333

[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - New best model: score = 0.40150740132486906, epoch = 10000 (previous: score = 0.4021621963241919, epoch = 9900)
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 10000. Details: MaxEpochsTerminationCondition(10000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(10000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 10001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 10000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.40150740132486906
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.29],
 [1.00, 0.74],
 [1.00, 0.84],
 [1.00, 0.95],
 [0.00, 0.32],
 [0.00, 0.17],
 [1.00, 0.23],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.84],
 [1.00, 0.94],
 [0.00, 0.16],
 [0.00, 0.26],
 [0.00, 0.04],
 [0.00, 0.01]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8188
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.29],
 [1.00, 0.74],
 [1.00, 0.84],
 [1.00, 0.95],
 [0.00, 0.32],
 [0.00, 0.17],
 [1.00, 0.23],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.84],
 [1.00, 0.94],
 [0.00, 0.16],
 [0.00, 0.26],
 [0.00, 0.04],
 [0.00, 0.01]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8188
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000

[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(20000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 20001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 11000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.39751010830814903
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.01],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.09],
 [1.00, 0.57],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.94],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8406
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.32],
 [1.00, 0.80],
 [1.00, 0.82],
 [1.00, 0.97],
 [0.00, 0.29],
 [0.00, 0.22],
 [1.00, 0.24],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.81],
 [1.00, 0.95],
 [0.00, 0.20],
 [0.00, 0.24],
 [0.00, 0.01],
 [0.00, 0.01]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8188
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000

abnormal 300, xvia
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - New best model: score = 0.2776090035825734, epoch = 2000 (previous: score = 0.278429406093573, epoch = 1900)
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 2000. Details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 2001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 2000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.2776090035825734
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.01, 0.03],
 [0.97, 0.87],
 [0.01, 0.04],
 [0.86, 0.67],
 [0.01, 0.03],
 [0.00, 0.01],
 [0.96, 0.84],
 [0.07, 0.10],
 [0.59, 0.45],
 [0.06, 0.09],
 [0.55, 0.39],
 [0.01, 0.02],
 [0.11, 0.13],
 [0.01, 0.02],
 [0.31, 0.27]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7464
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.01, 0.03],
 [0.97, 0.87],
 [0.01, 0.04],
 [0.86, 0.67],
 [0.01, 0.03],
 [0.00, 0.01],
 [0.96, 0.84],
 [0.07, 0.10],
 [0.59, 0.45],
 [0.06, 0.09],
 [0.55, 0.39],
 [0.01, 0.02],
 [0.11, 0.13],
 [0.01, 0.02],
 [0.31, 0.27]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7464
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333

[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 10000
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 10000. Details: MaxEpochsTerminationCondition(10000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(10000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 10001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 2000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.2776090035825734
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.00],
 [0.00, 0.01],
 [0.00, 0.00],
 [0.72, 0.60],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.97],
 [0.06, 0.05],
 [0.21, 0.46],
 [0.00, 0.00],
 [1.00, 0.53],
 [0.00, 0.00],
 [0.00, 0.08],
 [0.00, 0.00],
 [0.00, 0.01]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8768
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.01, 0.03],
 [0.97, 0.87],
 [0.01, 0.04],
 [0.86, 0.67],
 [0.01, 0.03],
 [0.00, 0.01],
 [0.96, 0.84],
 [0.07, 0.10],
 [0.59, 0.45],
 [0.06, 0.09],
 [0.55, 0.39],
 [0.01, 0.02],
 [0.11, 0.13],
 [0.01, 0.02],
 [0.31, 0.27]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7464
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333




[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2000 is 0.27958013115663816
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 2000
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - New best model: score = 0.39287265041777164, epoch = 2000 (previous: score = 0.40234529789604717, epoch = 1900)
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 2000. Details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 2001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 2000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.39287265041777164
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.03, 0.02],
 [0.73, 0.57],
 [0.95, 0.90],
 [0.96, 0.92],
 [0.02, 0.01],
 [0.21, 0.14],
 [0.76, 0.59],
 [0.48, 0.34],
 [1.00, 0.99],
 [0.04, 0.02],
 [0.07, 0.04],
 [0.37, 0.24],
 [0.44, 0.30],
 [0.89, 0.81],
 [0.70, 0.54]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.6960
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.03, 0.02],
 [0.73, 0.57],
 [0.95, 0.90],
 [0.96, 0.92],
 [0.02, 0.01],
 [0.21, 0.14],
 [0.76, 0.59],
 [0.48, 0.34],
 [1.00, 0.99],
 [0.04, 0.02],
 [0.07, 0.04],
 [0.37, 0.24],
 [0.44, 0.30],
 [0.89, 0.81],
 [0.70, 0.54]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.6960
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000


[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 5000. Details: MaxEpochsTerminationCondition(5000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(5000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 5001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 2300
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.37337213591160645
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.01, 0.00],
 [0.17, 0.02],
 [1.00, 1.00],
 [0.69, 0.88],
 [0.03, 0.00],
 [0.03, 0.02],
 [0.74, 0.03],
 [0.79, 0.97],
 [1.00, 1.00],
 [0.41, 0.00],
 [0.01, 0.00],
 [0.05, 0.00],
 [0.08, 0.00],
 [0.98, 1.00],
 [0.85, 0.79]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7920
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.02, 0.01],
 [0.70, 0.52],
 [0.97, 0.91],
 [0.98, 0.95],
 [0.01, 0.00],
 [0.22, 0.13],
 [0.73, 0.49],
 [0.57, 0.45],
 [1.00, 1.00],
 [0.02, 0.01],
 [0.04, 0.02],
 [0.30, 0.17],
 [0.32, 0.18],
 [0.94, 0.90],
 [0.73, 0.56]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=300)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=300, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7200
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 9900 is 0.17685664021407532
[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 10000 is 0.17601844514668863
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 10000
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 10000. Details: MaxEpochsTerminationCondition(10000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(10000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 10001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 2500
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.3698203316592768
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.00],
 [0.02, 0.00],
 [1.00, 1.00],
 [0.98, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.40, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.55, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.99, 1.00],
 [1.00, 1.00]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8720
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.02, 0.01],
 [0.62, 0.44],
 [0.97, 0.93],
 [1.00, 1.00],
 [0.01, 0.00],
 [0.31, 0.20],
 [0.75, 0.50],
 [0.59, 0.54],
 [1.00, 1.00],
 [0.01, 0.00],
 [0.02, 0.01],
 [0.29, 0.17],
 [0.04, 0.01],
 [0.96, 0.94],
 [0.76, 0.63]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7360
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.7333

[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 20000 is 0.14541688857950638
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 20000
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 20000. Details: MaxEpochsTerminationCondition(20000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(20000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 20001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 2500
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.3698203316592768
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.00],
 [0.09, 0.01],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.98, 0.82],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.97, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8960
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8000



[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 30000 is 0.14189430365226532
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 30000
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - New best model: score = 0.2491456504735678, epoch = 30000 (previous: score = 0.2493269172882478, epoch = 29900)
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 30000. Details: MaxEpochsTerminationCondition(30000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(30000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 30001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 30000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.2491456504735678
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.00],
 [1.00, 0.93],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.99, 0.80],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8960
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8667
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.00],
 [1.00, 0.93],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.99, 0.80],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8960
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8667


[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 40000 is 0.1409607006978315
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 40000
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - New best model: score = 0.24262280619500232, epoch = 40000 (previous: score = 0.2426946477012649, epoch = 39900)
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 40000. Details: MaxEpochsTerminationCondition(40000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(40000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 40001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 40000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.24262280619500232
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.84],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8960
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8667
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.84],
 [1.00, 1.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8960
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.8667

