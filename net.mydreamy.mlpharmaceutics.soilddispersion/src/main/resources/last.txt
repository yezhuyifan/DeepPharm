node = 500

[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2000 is 0.20317642575067907
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 2000
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 2000. Details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(2000)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 2001
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.5009867451241445
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.73, 0.58],
 [0.21, 0.10],
 [0.00, 0.00],
 [0.04, 0.02],
 [0.94, 0.90],
 [1.00, 1.00],
 [0.97, 0.96],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.97, 0.95],
 [1.00, 1.00],
 [0.53, 0.35],
 [0.67, 0.48],
 [0.00, 0.00],
 [0.03, 0.01],
 [0.01, 0.00],
 [0.00, 0.00],
 [0.64, 0.45],
 [0.00, 0.00]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8350
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.5000
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.59, 0.52],
 [0.42, 0.34],
 [0.11, 0.08],
 [0.60, 0.53],
 [0.61, 0.53],
 [0.34, 0.27],
 [0.43, 0.34],
 [0.62, 0.54],
 [0.58, 0.49],
 [0.84, 0.80],
 [0.18, 0.13],
 [0.59, 0.51],
 [0.30, 0.23],
 [0.42, 0.33],
 [0.40, 0.32],
 [0.14, 0.10],
 [0.17, 0.13],
 [0.57, 0.48],
 [0.41, 0.32],
 [0.61, 0.52]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=500)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=500, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=Step, lrPolicyDecayRate=0.1, lrPolicySteps=100000.0, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.6990
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.5500


node = 200
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 1500
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 1500. Details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 1501
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1100
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.4756520765121118
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.70, 0.60],
 [0.16, 0.11],
 [0.01, 0.01],
 [0.26, 0.19],
 [0.74, 0.64],
 [0.20, 0.14],
 [0.71, 0.60],
 [0.96, 0.93],
 [0.08, 0.06],
 [0.99, 0.99],
 [0.09, 0.06],
 [0.94, 0.90],
 [0.18, 0.13],
 [0.10, 0.07],
 [0.07, 0.05],
 [0.01, 0.01],
 [0.01, 0.01],
 [0.04, 0.03],
 [0.10, 0.07],
 [0.19, 0.13]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8058
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.5500
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.61, 0.51],
 [0.36, 0.29],
 [0.09, 0.07],
 [0.60, 0.50],
 [0.62, 0.53],
 [0.32, 0.25],
 [0.48, 0.39],
 [0.68, 0.59],
 [0.55, 0.46],
 [0.87, 0.80],
 [0.14, 0.11],
 [0.66, 0.56],
 [0.31, 0.25],
 [0.36, 0.28],
 [0.40, 0.32],
 [0.11, 0.09],
 [0.18, 0.14],
 [0.51, 0.41],
 [0.34, 0.27],
 [0.58, 0.48]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=200)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=200, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7282
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.5000



50 node

[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 1500. Details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 1501
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1200
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.4531652306050242
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.72, 0.64],
 [0.14, 0.09],
 [0.05, 0.03],
 [0.36, 0.27],
 [0.69, 0.60],
 [0.09, 0.06],
 [0.62, 0.53],
 [0.80, 0.74],
 [0.17, 0.11],
 [0.92, 0.90],
 [0.08, 0.05],
 [0.74, 0.66],
 [0.20, 0.14],
 [0.10, 0.06],
 [0.19, 0.13],
 [0.05, 0.03],
 [0.05, 0.03],
 [0.12, 0.08],
 [0.10, 0.06],
 [0.26, 0.19]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.8058
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.5500
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.61, 0.55],
 [0.32, 0.25],
 [0.13, 0.09],
 [0.52, 0.44],
 [0.61, 0.54],
 [0.24, 0.17],
 [0.52, 0.44],
 [0.59, 0.52],
 [0.50, 0.42],
 [0.78, 0.74],
 [0.17, 0.12],
 [0.55, 0.47],
 [0.33, 0.26],
 [0.33, 0.25],
 [0.45, 0.37],
 [0.14, 0.10],
 [0.15, 0.11],
 [0.46, 0.38],
 [0.32, 0.24],
 [0.55, 0.47]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=XAVIER, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.7184
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.5500


 
                .weightInit(WeightInit.RELU)


[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 1497
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 1498
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 1499
[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 1500 is 0.06099260498681422
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Completed training epoch 1500
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - New best model: score = 0.35683625468545055, epoch = 1500 (previous: score = 0.35750833686123884, epoch = 1400)
[main] INFO org.deeplearning4j.earlystopping.trainer.BaseEarlyStoppingTrainer - Hit epoch termination condition at epoch 1500. Details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination reason: EpochTerminationCondition
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Termination details: MaxEpochsTerminationCondition(1500)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Total epochs: 1501
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Best epoch number: 1500
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - Score at best epoch: 0.35683625468545055
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== testing =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== latest model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.96, 0.93],
 [0.07, 0.05],
 [0.14, 0.03],
 [0.02, 0.01],
 [0.85, 0.77],
 [0.92, 0.80],
 [0.94, 0.32],
 [0.66, 0.45],
 [0.91, 0.50],
 [0.93, 0.88],
 [0.97, 0.96],
 [0.72, 0.39],
 [0.03, 0.03],
 [0.03, 0.03],
 [0.05, 0.03],
 [0.29, 0.16],
 [0.04, 0.02],
 [0.02, 0.02],
 [0.04, 0.03],
 [0.60, 0.21]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9223
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6500
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - ========================== best model =========================
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test label value: 
[[1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 0.00],
 [1.00, 1.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00],
 [0.00, 0.00],
 [1.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [0.00, 0.00],
 [1.00, 1.00]]
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test prediction value: 
[[0.96, 0.93],
 [0.07, 0.05],
 [0.14, 0.03],
 [0.02, 0.01],
 [0.85, 0.77],
 [0.92, 0.80],
 [0.94, 0.32],
 [0.66, 0.45],
 [0.91, 0.50],
 [0.93, 0.88],
 [0.97, 0.96],
 [0.72, 0.39],
 [0.03, 0.03],
 [0.03, 0.03],
 [0.05, 0.03],
 [0.29, 0.16],
 [0.04, 0.02],
 [0.02, 0.02],
 [0.04, 0.03],
 [0.60, 0.21]]
[main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork -  Layer 0 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=15, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 1 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 2 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 3 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 4 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 5 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 6 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 7 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 8 conf NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=tanh, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=50)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN) Layer 9 conf NeuralNetConfiguration(layer=OutputLayer(super=BaseOutputLayer(super=FeedForwardLayer(super=Layer(layerName=null, activationFunction=sigmoid, weightInit=RELU, biasInit=0.0, dist=null, learningRate=0.01, biasLearningRate=0.01, learningRateSchedule=null, momentum=0.8, momentumSchedule={}, l1=0.0, l2=0.001, biasL1=0.0, biasL2=0.0, dropOut=0.0, updater=NESTEROVS, rho=NaN, epsilon=NaN, rmsDecay=NaN, adamMeanDecay=NaN, adamVarDecay=NaN, gradientNormalization=RenormalizeL2PerLayer, gradientNormalizationThreshold=1.0), nIn=50, nOut=2), lossFn=LossL2(), customLossFunction=null)), leakyreluAlpha=0.0, miniBatch=true, numIterations=1, maxNumLineSearchIterations=5, seed=1234567890, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[W, b], stepFunction=null, useRegularization=true, useDropConnect=false, minimize=true, learningRateByParam={b=0.01, W=0.01}, l1ByParam={b=0.0, W=0.0}, l2ByParam={b=0.0, W=0.001}, learningRatePolicy=None, lrPolicyDecayRate=NaN, lrPolicySteps=NaN, lrPolicyPower=NaN)
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - train 3-6 correctness: 0.9223
[main] INFO net.mydreamy.mlpharmaceutics.soliddispersion.TranningSolidDispersion3and6 - test 3-6 correctness: 0.6500



